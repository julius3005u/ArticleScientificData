Super-resolution (SR) aims to reconstruct high-resolution (HR) signals from low-resolution (LR) observations. Deep learning methods have advanced this task, yet they rely on abundant HR data that can be scarce, costly or hard to obtain. This study investigates the use of synthetic data to train SR models for one-dimensional (1D) signals. Using EEG recordings and synthetically generated signals, we evaluate four training strategies: training on the real dataset only (Real), training exclusively with synthetic data (Synthetic), training on both synthetic and real data jointly (Mixed), and synthetic pretraining followed by real fine-tuning (Tunned).

Synthetic-only models perform worst across datasets, while combining or pretraining with synthetic data improves results substantially. On EEG validation data, the Mixed model reduces mean absolute error (MAE) by 9.64\% relative to the Real baseline; on the out-of-domain VCTK speech dataset, the Tunned model achieves a 25.51\% reduction. These findings show that synthetic data effectively augment limited real datasets, enhancing generalization and robustness in SR tasks.