\documentclass[fleqn,10pt]{wlscirep}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lineno}
 \usepackage{multirow}
\usepackage{array}  % Añade esto en el preámbulo
\renewcommand{\arraystretch}{1.3}  % Espaciado vertical entre filas

\usepackage{subcaption}
\linenumbers
\usepackage{float}

% ===== TRACK CHANGES PACKAGE =====
% Use [draft] to show changes with highlighting
% Use [final] to hide all markup for submission
\usepackage[draft]{changes}
\usepackage{xcolor}

% Define authors for tracked changes
\definechangesauthor[name={Revision}, color=blue]{REV}

% Custom commands for easier tracking
\newcommand{\addtext}[1]{\added[id=REV]{#1}}
\newcommand{\deltext}[1]{\deleted[id=REV]{#1}}
\newcommand{\replacetext}[2]{\replaced[id=REV]{#2}{#1}}
\newcommand{\notetext}[1]{\comment[id=REV]{#1}}
\newcommand{\highlighttext}[1]{\highlight[id=REV]{#1}}
% ===== END TRACK CHANGES ===== 

%\title{CoSiBD: A Versatile Dataset for Benchmarking Signal Processing and Machine Learning Algorithms}
%\title{CoSiBD: A Synthetic Dataset for Super-Resolution Deep Learning Temporal Series Analysis}
%\title{A synthetic dataset for super-resolution of Time Series using Deep Learning}
\title{A synthetic dataset for Time Series Super-Resolution with Deep Learning}
\author[1]{Julio Ibarra-Fiallo}
\author[2]{Juan A. Lara}
\author[1]{D'hamar Agudelo-Moreno}
%\author[2,*]{autor }
\affil[1]{Colegio de Ciencias e Ingenierías, Universidad San Francisco de Quito, Cumbayá, Ecuador}
\affil[2]{Universidad de Córdoba, Córdoba, España}

\affil[*]{corresponding author: Julio Ibarra-Fiallo (jibarra@usfq.edu.ec)}

\begin{abstract}
\replacetext
{The increasing application of temporal signal analysis in fields like biomedical engineering, telecommunications, and industrial monitoring emphasizes the need for high-quality data to train and evaluate advanced machine learning models.}
{The increasing application of time-series analysis in fields such as biomedical engineering, telecommunications, and industrial monitoring emphasizes the need for high-quality datasets to develop, compare, and validate data-driven methods.}
Acquiring real-world temporal data at suitable resolutions is often limited by ethical, economic, or practical constraints. To address this, we introduce CoSiBD (Complex Signal Benchmark Dataset for Super-Resolution), a synthetic dataset of complex temporal signals
\replacetext
{designed for training and assessing AI models, particularly deep learning systems, in tasks like temporal super-resolution and signal processing.}
{designed to support reproducible research in multi-resolution time-series analysis, including temporal super-resolution and related signal processing tasks.}
\addtext{CoSiBD comprises 2,500 high-resolution signals ($N=5{,}000$ samples each over a reference domain $\tau\in[0,4\pi]$), with corresponding low-resolution versions provided at four target sampling levels (150, 250, 500, and 1,000 samples) obtained via uniform decimation of the original sequences.}
CoSiBD includes diverse signals with non-uniform frequency modulations, capturing gradual transitions and abrupt high-frequency events
\replacetext
{to mirror real-world dynamics.}
{to reflect a broad range of non-stationary temporal behaviors.}
\replacetext
{It offers signals at multiple resolutions with varying noise levels, enabling robust evaluation of model performance under realistic conditions, especially for super-resolution tasks.}
{The dataset includes clean and noisy variants across all sampling resolutions, enabling systematic benchmarking under controlled variability conditions.}
The dataset is generated by combining distinct frequency bands, non-uniform intervals, and probabilistic frequency assignments to create realistic patterns, with smoothing achieved through spline interpolation.
\replacetext
{Validated for spectral consistency across sampling rates and noise, CoSiBD supports training and evaluation.}
{Technical validation focuses on the spectral characteristics of the generated signals across sampling resolutions and noise settings, documenting consistency and controlled variability under the reported generation parameters.}
\end{abstract}





\begin{document}

\flushbottom
\maketitle

\section*{Background \& Summary}
\label{sec:background-summary}

The analysis and simulation of temporal signals are fundamental across science and engineering, 
\deltext{supporting insights into dynamic processes.}
\addtext{providing critical insights into dynamic processes across multiple domains.}
In biomedical research \cite{Karacan2024}, electroencephalography (EEG) and electrocardiography (ECG) analyses reveal brain and heart function \cite{Nayak2023,shaffer2017}. 
Telecommunications rely on signal processing to ensure data fidelity across noisy media \cite{Chen2022}, while finance uses time-series forecasting for risk and trend analysis \cite{Zhang2016}. 
Industrial monitoring detects equipment faults using temporal patterns \cite{Bhatia2021}, and environmental science applies similar techniques to 
\deltext{climate tracking via remote sensing}
\addtext{climate and environmental monitoring using remote-sensor time series} \cite{Mallat1989}. 
Developing robust tools for interpreting time-varying data continues to support both scientific discovery and practical applications,
\addtext{while increasingly relying on the availability of reliable and well-characterized temporal signal datasets.}
\\ \\ 
Recent advances in deep learning have contributed significantly to this field by enabling automatic extraction of complex features from raw signals. 
Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), including Long Short-Term Memory (LSTM) units, and Generative Adversarial Networks (GANs) have demonstrated improved performance over traditional techniques in image, speech, and time-series processing tasks \cite{Lecun2015,Goodfellow2014}. 
These models support fine-grained signal reconstruction and forecasting, allowing researchers to explore temporal dynamics in new ways,
\addtext{but also increasing the demand for well-structured, high-quality temporal signal datasets suitable for training and evaluation.}
\\ \\
Despite this progress, deep learning methods for temporal signal processing often require large quantities of labeled, high-quality data. 
Access to such data is frequently constrained by medical privacy regulations such as the General Data Protection Regulation (GDPR) and the Health Insurance Portability and Accountability Act (HIPAA) \cite{Isasa2024}. 
In other domains, including 
\deltext{remote sensing and industrial monitoring}
\addtext{environmental monitoring using remote sensors and industrial monitoring}, 
data availability is limited by practical and economic barriers to sensor deployment and data collection \cite{Zhang2016}. 
These limitations are particularly relevant in super-resolution (SR) tasks, where models require paired low- and high-resolution signals for effective training,
\addtext{which are rarely available in sufficient quantity and with consistent acquisition conditions.}
\\ \\
Temporal SR, which enhances resolution over time, has broad potential. 
\deltext{In medicine, for instance, it improves magnetic resonance imaging (MRI) and computed tomography (CT) scans, supporting earlier disease detection \cite{Morales2022}.}
\addtext{In biomedical monitoring and sensing, temporal SR can help reconstruct higher-resolution physiological time series, such as ECG and EEG signals.}
For EEG analysis, SR may help recover high-frequency components that aid in the study of neural oscillations \cite{Nayak2023} or detect subtle physiological irregularities \cite{shaffer2017}. 
\deltext{In remote sensing, SR helps refine satellite imagery \cite{Mallat1989},}
\addtext{In domains such as environmental sensing, telecommunications, and industrial monitoring,}
SR \addtext{can increase sensitivity to rapid temporal changes,} 
\deltext{while in telecommunications it contributes to enhanced signal reliability. It also has applications in industrial monitoring by increasing sensitivity to system changes.}
\\ \\
Traditional SR methods such as polynomial interpolation, frequency-domain transforms, and splines each have limitations. Polynomial models are often insufficient for capturing nonlinear dynamics; frequency-domain methods are susceptible to noise \cite{Mallat1989}; and splines, though flexible, may not generalize well to complex signal variability \cite{Schumaker2007,DeBoor2001}. Many of these methods also assume uniform partitioning, which may not align with the multi-scale, irregular structure of natural temporal phenomena.
\\ \\
Deep learning offers adaptive alternatives to these traditional methods. CNNs are capable of modeling spatio-temporal structure, RNNs and LSTMs capture long-range dependencies in time, and GANs can learn high-resolution representations through adversarial training \cite{Lecun2015,Goodfellow2014}. While GANs have achieved strong results in image SR \cite{Brophy2023}, their application to time-series SR remains relatively new. Preliminary work on synthetic time-series generation indicates potential \cite{Brophy2023}, but the lack of accessible, high-quality paired datasets remains a significant barrier to progress.
\\ \\
Synthetic datasets offer one solution to this problem, allowing researchers to design reproducible training environments that reflect the structure and variability of real-world signals. Prior studies have used synthetic data in domains such as fluid dynamics \cite{Yasuda2023}, bioimaging \cite{Priessner2024}, and live-cell imaging \cite{Qiao2025}, demonstrating that synthetic approaches can help simulate complexity while avoiding legal and practical restrictions associated with real-world data.
\\ \\
To support research in super-resolution for time-series data, we present the Complex Signal Benchmark Dataset (CoSiBD).
CoSiBD is a synthetic dataset composed of time-series signals with variable resolution, frequency characteristics, and noise levels.
\replacetext{The dataset is intended to provide a resource for training and evaluating SR models under controlled, reproducible conditions.}
{The dataset is intended to provide a reusable benchmark resource for the development and comparison of super-resolution methods under controlled and reproducible conditions.}
It includes 
\deltext{non-uniformly sampled signals,}
\addtext{non-stationary, piecewise-structured signals generated via non-uniform interval partitioning with change-points,}
multiple levels of resolution and noise, a technical validation suite, and publicly available Python code to facilitate use.
\deltext{CoSiBD has been used in research presented at the International Conference on Signal Processing and Machine Learning and is made available to support further development in deep learning approaches for temporal super-resolution.}

\subsection*{Related synthetic time-series resources}

\addtext{Publicly available synthetic resources for temporal signals exist, but they are typically designed for tasks other than time-series super-resolution (SR), or they target a specific domain. In wireless communications, the RadioML family provides large collections of synthetic complex I/Q sequences with varying signal-to-noise ratios and channel impairments, mainly to benchmark automatic modulation classification rather than paired SR reconstruction \cite{oshea2016grcon,deepsig_datasets,deepsig_radioml2018}. In biomedical signal processing, physiological simulators such as ECGSYN (ECG) and SEREEGA (EEG) enable controlled generation with tunable morphology, sampling settings, and noise, supporting method development when access to real data is constrained \cite{mcsharry2003ecg,ecgsyn_physionet,krol2018sereega}. In power systems, LoadGAN provides multi-resolution generation of load time series across sampling rates and time horizons, but it is not distributed as a standardized paired SR benchmark \cite{pinceti2021loadgan}. Domain-specific paired low- and high-resolution training data can also be produced via physical forward modeling, for example low- and high-resolution one-dimensional seismic traces for learning-based resolution enhancement \cite{yuan2024seismic}.}
\\ \\
\noindent
\addtext{Table~\ref{tab:related_synthetic_datasets} summarizes these representative resources and highlights a practical gap: while many tools provide synthetic signals, they usually do not jointly offer (i) multi-factor paired low- and high-resolution signals suitable for time-series SR, (ii) a clearly specified and reproducible protocol for constructing low-resolution observations aligned to a fixed high-resolution target, and (iii) per-signal metadata enabling deterministic regeneration and principled benchmarking. CoSiBD is designed to address this gap by providing multi-resolution paired signals, explicit nuisance modeling (including noise and structured interference), and comprehensive metadata for reproducible super-resolution benchmarking across multiple difficulty levels.}

\begin{table*}[t]
\centering
\begingroup
\small
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{|p{2.7cm}|p{2.3cm}|p{1.6cm}|p{1.9cm}|p{1.7cm}|p{2.2cm}|p{2.4cm}|}
\hline
\textbf{Resource} & \textbf{Domain} & \textbf{Form} & \textbf{Paired LR--HR SR} & \textbf{Multi-resolution} & \textbf{Noise / artifacts} & \textbf{Reproducibility granularity} \\
\hline
\textbf{CoSiBD (this work)} &
Generic time series (complex-structured signals) &
Dataset + generator &
Yes (LR $\rightarrow$ HR targets) &
Yes ($150/250/500/1000 \rightarrow 5000$) &
Gaussian + structured interference; primary benchmark uses direct decimation &
Per-signal metadata; deterministic regeneration (seed-controlled) \\
\hline
RadioML 2016.10A \cite{oshea2016grcon,deepsig_datasets} &
Wireless communications (I/Q) &
Dataset &
No (classification benchmark) &
N/A (not SR) &
Variable SNR + channel impairments &
Dataset-level (labels/SNR) \\
\hline
RadioML 2018.01A \cite{deepsig_radioml2018} &
Wireless communications (I/Q) &
Dataset &
No (classification benchmark) &
N/A (not SR) &
Simulated channel effects + SNR variability &
Dataset-level \\
\hline
ECGSYN \cite{mcsharry2003ecg,ecgsyn_physionet} &
ECG (physiology) &
Simulator/tool &
Configurable\footnotemark[1] &
Configurable &
Model-based; supports controlled variability &
User-defined simulator parameters \\
\hline
SEREEGA \cite{krol2018sereega} &
EEG (physiology) &
Simulator/tool &
Configurable\footnotemark[1] &
Configurable &
Supports noise and event-related components &
User-defined simulator parameters \\
\hline
LoadGAN \cite{pinceti2021loadgan} &
Power systems load time series &
Generator/tool &
No (generation) &
Yes (variable sampling rates) &
Domain-specific variability &
Configurable generation settings \\
\hline
Synthetic LR--HR seismic traces \cite{yuan2024seismic} &
Seismic traces (geophysics) &
Paper-specific paired data &
Yes (LR--HR pairs) &
Study-specific &
Study-dependent &
Study-specific \\
\hline
\end{tabular}
\endgroup
\caption{Representative publicly available synthetic time-series datasets and simulators related to signal processing and learning.}
\label{tab:related_synthetic_datasets}
\end{table*}

\footnotetext[1]{``Configurable'' indicates that low- and high-resolution signals can be generated or derived by adjusting simulator settings or sampling rates, but a standardized paired super-resolution benchmark is not distributed as part of the resource.}


\section*{Methods}
\label{sec:methods}

The methodology used to generate the synthetic temporal signals that constitute the CoSiBD dataset is illustrated in Figure~\ref{fig:generation_process}. The signal generation process is designed to produce time series exhibiting structural properties commonly observed in real-world temporal data, including variable frequency content, smooth transitions, and intermittent high-frequency activity. A key aspect of the procedure is the generation of signals at multiple temporal resolutions, enabling the construction of paired datasets for super-resolution (SR) benchmarking.
\\ \\
\begin{figure}
    \centering
    \deleted{\includegraphics[width=0.35\textwidth]{diagrams/generation_process3.png}}
    \added{\includegraphics[width=0.35\textwidth]{diagrams/generation_process4.png}}
    \caption{Schematic overview of the CoSiBD signal generation process.}
    \label{fig:generation_process}
\end{figure}

\addtext{
\noindent\textbf{Signal design principles.}
The CoSiBD signal generator incorporates structural properties commonly observed in physiological and speech time series, including (i) non-stationary regime changes, (ii) coexisting low- and high-frequency components with intermittent transients, (iii) smooth amplitude-envelope evolution, and (iv) baseline drift and measurement noise. These properties are implemented through non-uniform interval partitioning with change-points, separate low- and high-frequency bands, spline-based amplitude envelopes and frequency profiles, and explicit offset and noise terms. Figure~\ref{fig:design_rationale_motivations} illustrates representative examples of these signal characteristics and the corresponding design mechanisms used in CoSiBD.
}
\\ \\
\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{graphs/r1_3_design_rationale/r1_3_real_signal_motivations.png}
    \caption{\addtext{
Representative examples of non-stationary temporal properties in physiological and speech signals that informed the CoSiBD design. The figure shows regime changes, structured spectral content, amplitude-envelope dynamics, and smoothly varying frequency trends, which are reflected in the corresponding signal generation mechanisms.
}}
    \label{fig:design_rationale_motivations}
\end{figure}

\noindent The signal generation pipeline involves the following steps:

\begin{enumerate}
    \item \textbf{Base frequency band definition:}
    A set of distinct frequency bands is defined to represent the underlying spectral content of the signals.
    These can be adjusted to reflect application-specific characteristics.

    \item \textbf{Non-uniform interval partitioning:}
    The total signal duration is divided into multiple intervals of variable length.
    The interval lengths are determined probabilistically to introduce variability in the signal structure
    \addtext{and non-stationarity through change-points.}

    \item \textbf{Frequency assignment:}
    Each interval is assigned a dominant frequency band, sampled according to a predefined probability distribution.
    This introduces spectral variation over time.

    \item \textbf{Signal synthesis:}
    A sinusoidal waveform, or a combination of sinusoids within the assigned frequency band, is generated for each interval.
    Signal parameters such as amplitude and phase are configurable.

    \item \textbf{Transition smoothing:}
    To avoid discontinuities at interval boundaries, a smoothing function is applied to overlapping segments.
    This ensures gradual transitions between intervals with different frequency content.
    
    \item \textbf{Resolution variation:}
    All signals are initially synthesized at a high temporal resolution.
    Lower-resolution versions are created by applying controlled downsampling to the high-resolution signals, forming paired datasets.
    \addtext{In CoSiBD, paired low-resolution sequences are obtained via simple uniform decimation (uniform subsampling) of the high-resolution signals. The low-resolution observation is formed by subsampling the original sequence without pre-filtering.}

    \item \textbf{Noise injection:}
    Controlled levels of synthetic noise are added to the signals to emulate different data acquisition scenarios.
    Both the type and intensity of the noise can be configured.
    \addtext{Two noise types are implemented: additive Gaussian noise with configurable amplitude and structured sinusoidal interference. Noise is applied probabilistically on a per-signal basis. All noise parameters are recorded in per-signal metadata.}

 \end{enumerate}

\addtext{\noindent\textbf{Rationale for structured 50/60\,Hz interference and noise.} Real measurement pipelines frequently contain narrow-band interference (e.g., mains hum) superimposed on broadband sensor noise. To reflect this common acquisition artifact, CoSiBD includes an optional structured sinusoidal component in addition to Gaussian noise. CoSiBD signals are generated over a reference domain (by default $\tau\in[0,4\pi]$); interpreting $\tau$ as physical time (and therefore reporting frequencies in Hz) requires an explicit time scaling. Throughout this manuscript we adopt an illustrative convention that maps the reference domain to a duration $T=4\pi$ seconds, under which the structured component can be interpreted as a 50/60\,Hz-like powerline interference term, while the broadband term represents the measurement noise floor. Figure~\ref{fig:r1_4_powerline_noise} illustrates this qualitative motivation; the intent is not to reproduce a specific device transfer function but to include realistic nuisance factors that SR models must handle.} % R1-4
\\ \\
\begin{figure}
    \centering
    \includegraphics[width=0.85\textwidth]{figures/powerline_interference_justification.png}
    \caption{Qualitative motivation for the structured interference term used in CoSiBD. An illustrative example shows how adding a narrow-band sinusoidal component (interpretable as 50/60\,Hz under the illustrative convention $T=4\pi$\,s) produces the characteristic periodic contamination observed in real recordings, while broadband noise captures the measurement floor.} % R1-4
    \label{fig:r1_4_powerline_noise}
\end{figure}

\addtext{\noindent\textbf{Sampling units and frequency interpretation.} CoSiBD signals are provided as discrete sequences $x[n]$ (e.g., $N=5{,}000$ samples) that are directly used as inputs/targets by SR models. The internal generation domain $\tau\in[0,4\pi]$ is a reference parameterization; interpreting it as physical time requires choosing a duration $T$ (in seconds) for the reference interval. Under this convention, the implied sampling rate is $f_s = N/T$ and all frequencies reported in Hz scale linearly with $4\pi/T$. Throughout this manuscript, when reporting example frequencies in Hz we adopt the illustrative convention $T=4\pi$\,s, yielding $f_s \approx 5000/(4\pi) \approx 398\,$Hz; other equally valid mappings exist depending on application. Consequently, any band-specific interpretation in Hz (e.g., ``low/high'' frequency ranges) should be understood under the chosen $T$; changing $T$ rescales all reported Hz values while preserving the underlying discrete sequences, which is a key feature of CoSiBD's reference-domain design. Figure~\ref{fig:r1_5_sampling_units} illustrates that the discrete samples are unchanged under different time scalings and that Hz axes shift with the assumed $f_s$, while the normalized spectrum (cycles/sample) is invariant.} % R1-5
\\ \\
\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{graphs/r1_5_sampling_units/r1_5_time_scaling.png}
    \vspace{0.25cm}
    \includegraphics[width=0.95\textwidth]{graphs/r1_5_sampling_units/r1_5_spectrum_mapping.png}
    \caption{Sampling/unit convention in CoSiBD. Top: the same discrete sequence $x[n]$ can be plotted against the sample index or under different assumed time scalings. Bottom: the intrinsic frequency axis is normalized (cycles/sample); mapping to ``Hz'' depends on the assumed sampling rate $f_s$ (two example mappings shown).} % R1-5
    \label{fig:r1_5_sampling_units}
\end{figure}

\noindent The parameters that govern each step of the generation process—such as interval length distributions, frequency band selection probabilities, smoothing function characteristics, sampling rates, and noise settings—can be configured to produce signal sets tailored to different domains or experimental conditions.
\addtext{All generation parameters, including random seeds, are documented in comprehensive metadata (\texttt{signals\_metadata.json}), enabling exact reproduction of individual signals or the complete dataset. The generation pipeline is implemented in modular Python code available in the SignalBuilderC package, with clear interfaces for customization and extension.}
These configurations are included in the dataset's accompanying code to support reproducibility and allow users to regenerate the signals under consistent conditions.


\section*{Data Records}
\label{sec:data-records}

The Complex Signal Benchmark Dataset (CoSiBD) is publicly available
\addtext{on Zenodo}
and consists of synthetic temporal signals created to
support the development and evaluation of temporal super-resolution (SR) algorithms.
\addtext{The dataset is released under an open license to facilitate reuse and reproducibility.}
This section provides an overview of the
dataset structure, content, and storage format.
\\ \\
\noindent
\deltext{The dataset includes a total of 7,800 signal samples divided into two main categories:}
\addtext{The dataset comprises 2,500 high-resolution signals, each with corresponding subsampled versions at four resolution levels, organized into multiple categories:}

\begin{itemize}
    \item \deltext{High-resolution signals, generated at full sampling rate.}
    \addtext{\textbf{High-resolution signals}: 2,500 signals generated at full sampling rate, each consisting of 5,000 samples spanning the reference domain $[0,4\pi]$. Each signal is provided in three formats: NumPy compressed archives (.npz), plain-text files (.txt), and JSON representations (.json). Per-signal generative metadata---including frequency profiles with explicit change-points (\texttt{base\_points}, \texttt{high\_freq\_points}), segment labels (\texttt{variation\_type}), amplitude envelopes, spline parameters, vertical offsets, noise configurations, and random seeds---is stored in a consolidated metadata file (\texttt{signals\_metadata.json}), enabling exact regeneration of individual signals.}

    \item \deltext{\textbf{Low-resolution signals}, obtained through controlled downsampling of the high-resolution versions, available at three distinct resolution levels.}
    \addtext{\textbf{Simple subsampled signals}: low-resolution versions obtained via uniform decimation of the high-resolution signals at four target resolutions (150, 250, 500, and 1,000 samples). These paired low-resolution sequences are intended as inputs for temporal super-resolution benchmarking against the original 5,000-sample targets and are provided in .npz, .txt, and .json formats.}
\end{itemize}

\deltext{Noise is applied to both high- and low-resolution signals at different signal-to-noise ratio (SNR) levels (20 dB, 10 dB, and 5 dB), integrated directly into the signal files.}
\addtext{Reproducibility is ensured through documented random seeds: each high-resolution signal is generated using a unique seed (ranging from 10,000 to 12,499), enabling exact regeneration of individual signals or the entire dataset. All generation parameters are stored in metadata JSON files, allowing users to reproduce signals deterministically without relying on a fixed global seed.}
\\ \\
\noindent
\addtext{The dataset is provided as consolidated files organized by resolution level and processing method. High-resolution signals are stored as \texttt{signals\_\allowbreak high\_\allowbreak resolution\_\allowbreak 5000.[npz|txt|json]}. Simple subsampled (uniformly decimated) signals are stored as \texttt{signals\_\allowbreak subsampled\_\allowbreak simple\_\allowbreak \{150,250,500,1000\}.[npz|txt|json]}. Dataset-level metadata and configuration files are provided separately, including \texttt{signals\_\allowbreak metadata.json} and \texttt{dataset\_\allowbreak summary.json}.}
\\ \\
\noindent
\deltext{Signals are stored in plain text `.txt` files containing NumPy-formatted arrays. Each file represents a single temporal signal as a one-dimensional sequence of numerical values. The dataset folder structure mirrors the subset naming scheme.}
\addtext{Each signal is provided in three formats: (1) NumPy compressed format (.npz) containing the signal array, the corresponding time grid, and (for high-resolution records only) the clean signal without noise; (2) plain-text format (.txt), where signals are stored as numerical arrays with samples separated by whitespace, for maximum portability; and (3) JSON format (.json) containing both time and signal arrays to support web-based applications and interoperability. Per-signal generative metadata is stored separately in \texttt{signals\_metadata.json} (one entry per signal), while dataset-level configuration and summary information is provided in \texttt{dataset\_summary.json}.}

\subsection*{Metadata schema and example}

\addtext{CoSiBD provides per-signal metadata to support (i) deterministic regeneration, (ii) principled partitioning (e.g., by noise type/level or segment labels), and (iii) analysis of the piecewise structure induced by change-points. Table~\ref{tab:metadata_schema} summarizes representative fields contained in \texttt{signals\_metadata.json}. A minimal example entry is shown below (one signal; values truncated for brevity).}

\begin{table}[H]
\centering
\small
\begin{tabular}{|p{3.2cm}|p{3.2cm}|p{7.2cm}|}
\hline
\textbf{Field} & \textbf{Type / example} & \textbf{Meaning} \\ \hline
\texttt{signal\_id}, \texttt{index} & \texttt{"signal\_0000"}, 0 & Unique identifier and row index used to align LR--HR pairs across consolidated files. \\ \hline
\texttt{seed} & 10000--12499 & Per-signal random seed enabling deterministic regeneration. \\ \hline
\texttt{t\_start}, \texttt{t\_end} & 0.0, 12.566... & Reference-domain interval (\(\tau\in[0,4\pi]\)) used by the generator. \\ \hline
\texttt{fs\_high} & 397.887... & Illustrative sampling rate under the manuscript convention \(T=4\pi\) s (see Sampling units and frequency interpretation). \\ \hline
\texttt{tau\_frequency} & 1.15 & Frequency-profile spline tension parameter. \\ \hline
\texttt{amplitude\_\allowbreak spline\_\allowbreak type}, \texttt{tau\_\allowbreak amplitude} &
\texttt{"zero\_\allowbreak order"}, \texttt{"N/A"} &
Envelope model type and, when applicable, its tension parameter. \\ \hline
\texttt{base\_points} & \(K\times 2\) array & Change-points and base-band frequencies defining the piecewise frequency profile. \\ \hline
\texttt{high\_freq\_points} & \(K\times 2\) array & Change-points and high-frequency component levels (intermittent transients). \\ \hline
\texttt{variation\_type} & \texttt{["low", ...]} & Segment labels aligned to change-points (e.g., low/high/no-change regime). \\ \hline
\texttt{amp\_knots}, \texttt{amp\_values} & arrays & Envelope control knots and values. \\ \hline
\texttt{vertical\_offset} & 0.069... & Additive baseline drift term. \\ \hline
\texttt{noise\_profile} & JSON object & Noise flags and parameters (e.g., Gaussian vs structured interference; probabilities; amplitudes). \\ \hline
\end{tabular}
\caption{Representative per-signal metadata fields in \texttt{signals\_metadata.json}. The file contains one entry per signal, supporting deterministic regeneration and analysis/partitioning based on the signal's piecewise structure and nuisance settings.}
\label{tab:metadata_schema}
\end{table}

\begin{verbatim}
{
    "t_start": 0.0,
    "t_end": 12.566370614359172,
    "fs_high": 397.88735772973837,
    "tau_frequency": 1.15,
    "amplitude_spline_type": "zero_order",
    "vertical_offset": 0.06905161748158965,
    "base_points": [[0.0, 2.076409156965817], [1.9229451245119575, 2.076409156965817], ...],
    "high_freq_points": [[0.0, 0.0], [1.9229451245119575, 0.0], ...],
    "variation_type": ["low", "low", "low", "low"],
    "amp_knots": [0.0, 6.28192867011815, 12.5638573402363],
    "amp_values": [0.7237770021649202, 1.2266792057266251, 0.9661294815645534],
    "noise_profile": {"has_noise": true, "noise_type": "gaussian", "p_has_noise": 0.5, ...},
    "seed": 10000,
    "signal_id": "signal_0000",
    "index": 0
}
\end{verbatim}

\noindent The following resolution levels are available:

\begin{itemize}
    \item \textbf{High-resolution:} 5000 \deltext{points}\addtext{samples} per signal,
    \addtext{sampled over the reference domain $\tau\in[0,4\pi]$}.
    \addtext{An illustrative mapping to physical time is discussed in the Methods section.}

    \item \deltext{\textbf{Low-resolution:} Created via downsampling from the high-resolution version:}
    \addtext{\textbf{Subsampled resolutions:} Available as simple decimated versions of the high-resolution signals:}
    \begin{itemize}
        \item \deltext{1000 points}\addtext{1000 samples}
        \item \deltext{500 points}\addtext{500 samples}
        \item \deltext{250 points}\addtext{250 samples}
        \item \addtext{150 samples}
    \end{itemize}
\end{itemize}

\noindent Table~\ref{tab:Parameter} outlines the main parameters used in signal generation.
\deltext{Each signal was generated with randomly sampled values within the defined ranges.}
\addtext{Each high-resolution signal was generated using a unique random seed (ranging from 10{,}000 to 12{,}499), with parameter values randomly sampled within the defined ranges. This design supports dataset diversity while enabling exact regeneration of individual signals through the accompanying metadata.}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|m{6cm}|}
\hline
\textbf{Parameter} & \textbf{Range} & \textbf{Description} \\ \hline
Low Frequency & \replacetext{1--5 Hz}{1--5 (illustrative Hz for $T=4\pi$\,s)} & Low-frequency component present in signals \\ \hline
High Frequency & \replacetext{20--100 Hz}{20--100 (illustrative Hz for $T=4\pi$\,s)} & Higher-frequency variations for transitions \\ \hline
Change Points & 2--11 & Number of frequency transitions per signal \\ \hline
Change Locations & Random & Time locations where transitions occur \\ \hline
Variation Type & Categorical & Defines nature of frequency change ("low", "high", "no\_change") \\ \hline
\addtext{Amplitude Range} & \addtext{3--16} & \addtext{Range for amplitude envelope values} \\ \hline
\addtext{Vertical Offset} & \addtext{N(0, 3.0)} & \addtext{Normally distributed offset added to signals} \\ \hline
\addtext{Spline Type} & \addtext{Mixed} & \addtext{70\% zero-order (step), 30\% tension spline} \\ \hline
\addtext{Tension Parameter (freq)} & \addtext{[1, 2]} & \addtext{Tau values for frequency spline interpolation} \\ \hline
\addtext{Tension Parameter (amp)} & \addtext{Tau values for amplitude spline (when tension type)} \\ \hline
\addtext{Noise Probability} & \addtext{50\%} & \addtext{Probability of adding noise to each signal} \\ \hline
\addtext{Random Seed} & \addtext{10000--12499} & \addtext{Unique seed per signal for reproducibility} \\ \hline
\end{tabular}
\caption{Signal generation parameters used to create diverse temporal patterns within the CoSiBD dataset. \addtext{All parameters are documented in individual metadata files, enabling exact reproduction of each signal.} These parameters control the frequency composition and temporal structure.}
\label{tab:Parameter}
\end{table}

\noindent
\addtext{To explicitly characterize dataset diversity and complexity, CoSiBD spans multiple controlled axes of variation (Table~\ref{tab:Parameter}), including the number and location of change points, categorical transition types, low- and high-frequency bands, and amplitude-envelope configurations. This variability is visible in representative realizations (Figures~\ref{fig:amplitud} and~\ref{fig:simples}) and is further quantified in the Technical Validation section through the distribution of dominant frequencies (Figure~\ref{fig:dominant_frequency_distribution} and Table~\ref{tab:density_label}) and PSD behavior under different resolutions and noise settings (Figures~\ref{fig:average_psd} and~\ref{fig:noise_psd}). While the dataset is synthetic and not fitted to match a single domain-specific distribution, these controlled variations provide reproducible coverage of common real-world time-series phenomena, including non-stationarity, transient high-frequency events, and additive noise.}
\\ \\

\noindent Figure~\ref{fig:amplitud} shows a representative signal from the dataset sampled at different resolution levels. This illustrates the multi-resolution structure of CoSiBD and the alignment between high- and low-resolution representations.


\begin{figure}
\centering

% --- Row 1 ---
\begin{minipage}{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{graphs/amplitude_high.png}
    \label{fig:amp1}
\end{minipage}
\hspace{0.02\textwidth}
\begin{minipage}{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{graphs/amplitude_medium.png}
    \label{fig:amp2}
\end{minipage}

\vspace{0.15cm}

% --- Row 2 (centered single panel) ---
\begin{minipage}{0.6\textwidth}
    \centering
    \includegraphics[width=\textwidth]{graphs/amplitude_low.png}
    \label{fig:amp3}
\end{minipage}

\caption{A synthetic signal sampled at different resolutions: (a) high (5000 points), (b) medium (1000 points), and (c) low (150 points). These examples reflect the multi-resolution and noise conditions present in the dataset.}
\label{fig:amplitud}
\end{figure}

\vspace{0.3cm}

\noindent Figure~\ref{fig:simples} displays four additional synthetic signals generated using different configuration parameters. These examples demonstrate the variability in temporal structure across instances in the dataset.
\\ \\
\begin{figure}
\centering
\begin{minipage}{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{graphs/simples_high_low.png}
    \label{fig:simp1}
\end{minipage}
\hspace{0.02\textwidth}
\begin{minipage}{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{graphs/simples_localized.png}
    \label{fig:simp2}
\end{minipage}

\vspace{0.15cm}

\begin{minipage}{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{graphs/simples_smooth.png}
    \label{fig:simp3}
\end{minipage}
\hspace{0.02\textwidth}
\begin{minipage}{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{graphs/simples_irregular.png}
    \label{fig:simp4}
\end{minipage}

\caption{Examples of synthetic signals in the dataset generated with different parameter configurations. Each signal presents a distinct temporal profile.}
\label{fig:simples}
\end{figure}
\noindent The full dataset is publicly available on Zenodo \cite{cosibd_zenodo_2025} and includes the signal files and associated metadata organized in structured folders.


\section*{Technical Validation}
\label{sec:technical-validation}

\replacetext{
This section validates the proposed signal generation method by analyzing its spectral properties under different conditions, including the distribution of dominant frequencies, spectral stability across sampling rates, and the effect of noise. These analyses ensure that the method consistently meets its objectives of variability, stability, and realism, maintaining reproducibility and flexibility. Below, the methodologies and results are described in detail.
}{
This section characterizes the spectral properties of the generated signals under different conditions, including the distribution of dominant frequencies, spectral behavior across sampling rates, and the effect of noise. These analyses document how the dataset behaves under the reported generation settings, providing transparency for reproducibility and informed reuse. The applied procedures and observed outcomes are described in detail below.
}

\subsection*{Validation Context}

\deltext{
Experimental parameters were carefully selected to ensure reproducibility and
relevance. The number of signals was chosen to provide statistically significant
information about the variability and consistency of the generated signals. Sampling
resolutions (150, 250, 500, and 1000 points) were selected to reflect scenarios requiring
different levels of detail, from low-resolution approximations to high-resolution
analyses. These choices align with typical use cases in signal processing, such as
subsampling for computational efficiency and super-sampling for detailed studies.
\\ \\
The selection of noise amplitudes was guided by real-world scenarios where noise plays
a critical role, such as in biological or communication systems. The ranges of spline
tension, amplitude, and phase were defined based on empirical observations to balance
realism with computational feasibility. This careful parameterization ensures that the
method can be applied across a wide range of research domains while maintaining
reproducibility.
}
\addtext{
Experimental parameters were selected to support reproducibility and to document representative behaviors of the signal generator under the reported settings. Validation analyses were performed on a representative subset of signals to summarize common spectral trends, rather than to establish statistical significance. The provided scripts allow the same analyses to be replicated on any subset of the dataset.
\\ \\
Sampling resolutions (150, 250, 500, and 1000 samples) reflect scenarios requiring different levels of detail commonly encountered in signal processing workflows. Noise amplitudes and other parameter ranges were motivated by typical acquisition artifacts and exploratory checks, with the goal of providing a controllable benchmark for comparative evaluation rather than an exhaustive model of any specific measurement pipeline.
}

\subsection*{Analysis of Dominant Frequency Distribution}

\replacetext{
To assess the stability and variability of the primary spectral components, we analyzed the distribution of dominant frequencies
across multiple generated signals. A total of fifty independent signals were synthesized using identical input parameters. To
examine their spectral characteristics, we computed the power spectral density (PSD) of each signal, which quantifies how
signal power is distributed across different frequencies.
}{
To characterize the primary spectral components of the generated signals, we analyzed the distribution of dominant frequencies
across multiple independent realizations. A total of fifty signals were synthesized using identical generation settings. To
examine their spectral characteristics, we computed the power spectral density (PSD) of each signal, which quantifies how
signal power is distributed across frequencies.
}
\\ \\
\noindent
\replacetext{
The PSD was estimated using Welch’s method, selected for its ability to reduce noise and provide a smoother spectral
representation. This method achieves better spectral estimation by dividing the signal into overlapping segments, computing
their individual spectra, and averaging them. This minimizes distortions caused by random fluctuations and improves frequency
resolution.
}{
The PSD was estimated using Welch’s method \cite{Welch1967}, which stabilizes spectral estimation by dividing the signal into
overlapping segments, computing their individual spectra, and averaging them. This procedure reduces variance from random
fluctuations and yields a smoother spectral estimate.
}
For each signal, the dominant frequency was identified as the frequency at which the PSD reaches its maximum
value. This corresponds to the most prominent spectral component, indicating where the signal concentrates most of its energy.
\\ \\
\noindent
\replacetext{
By analyzing the distribution of dominant frequencies across the dataset, we evaluate whether the generated signals exhibit
consistent spectral patterns or if there is significant variation. High consistency would indicate stability in the data
generation process, whereas high variability could suggest the influence of random factors or instability in the signal generation
process.
}{
By analyzing the distribution of dominant frequencies across the dataset, we characterize the range and spread of the primary
spectral components produced by the generator. This analysis documents how dominant frequencies are distributed under fixed
generation settings, capturing both repeated structural patterns and the controlled variability introduced by randomized
parameters, without implying optimality or domain-specific realism.
}
\\ \\

\begin{figure}
    \centering
    \includegraphics[width=0.56\textwidth]{graphs/analysis_densidad.png}
    \caption{Distribution of dominant frequencies in 50 independently generated signals (reported in Hz under the illustrative convention $T=4\pi$\,s; for other choices of $T$, the Hz axis rescales by $4\pi/T$).}
    \label{fig:dominant_frequency_distribution}
\end{figure}

\begin{table}
\centering
\small
\begin{tabular}{|l|l|}
\hline
Statistic  & Value (Hz; illustrative $T=4\pi$\,s) \\
\hline
Average Dominant Frequency  & 0.508 \\
Standard Deviation  & 0.195 \\
Minimum Dominant Frequency  & 0.390 \\
Maximum Dominant Frequency  & 1.171 \\
\hline
\end{tabular}
\caption{\label{tab:density_label}Summary statistics of dominant frequencies, including average, standard deviation, and extreme values.}
\end{table}

\noindent
\deltext{
The results, shown in Figure~\ref{fig:dominant_frequency_distribution} and Table~\ref{tab:density_label}, demonstrate that the dominant frequencies are predominantly concentrated in
the low-frequency range (0.4 to 0.8 Hz), with sporadic occurrences of higher frequencies (1.1 to 1.2 Hz). This reflects the
method’s ability to generate signals with consistent primary structures while introducing controlled variability. Such flexibility is beneficial for applications requiring limited spectral variability while maintaining the predominance of low frequencies.
}
\addtext{
The results, shown in Figure~\ref{fig:dominant_frequency_distribution} and Table~\ref{tab:density_label}, show that dominant
frequency values are concentrated within a low normalized-frequency range, with occasional higher-frequency occurrences under
the same generation settings. These values are reported in normalized frequency units; conversion to physical units depends on
the chosen temporal scaling convention.
}
\\ \\
\noindent
\replacetext{
Figure~\ref{fig:noise_ruido} presents examples of signals from the CoSiBD dataset with increasing levels of added noise, illustrating how amplitude
fluctuations progressively obscure the underlying temporal structure.
}{
Figure~\ref{fig:noise_ruido} presents examples of signals from the CoSiBD dataset under increasing noise levels, illustrating how
added noise progressively obscures the underlying temporal structure.
}

\begin{figure}[H] \centering \begin{minipage}{0.47\textwidth} \centering \includegraphics[width=\textwidth]{graphs/ruido1.png} \label{fig:noise1} \end{minipage} \hspace{0.02\textwidth} \begin{minipage}{0.47\textwidth} \centering \includegraphics[width=\textwidth]{graphs/ruido2.png} \label{fig:noise2} \end{minipage}

\vspace{0.15cm}

\begin{minipage}{0.47\textwidth} \centering \includegraphics[width=\textwidth]{graphs/ruido3.png}  \label{fig:noise3} \end{minipage} \hspace{0.02\textwidth} \begin{minipage}{0.47\textwidth} \centering \includegraphics[width=\textwidth]{graphs/ruido4.png} \label{fig:noise4} \end{minipage}

\caption{Visualization of signals under increasing noise conditions, illustrating how added noise progressively masks the underlying temporal patterns present in the dataset. From low (a) to extreme noise levels (d), these examples document the effect of the reported noise settings on the signal structure.}
\label{fig:noise_ruido}
\end{figure}

\subsection*{Spectral Stability Across Sampling Resolutions}

\replacetext{
This analysis aims to investigate the influence of sampling resolution on the robustness of spectral estimates under varying
frequency content. At lower resolutions, aliasing can obscure critical frequency peaks, compromising the ability to distinguish
closely spaced spectral components. Conversely, higher resolutions improve the granularity of the frequency axis, allowing
for better separation of spectral features and reducing the risk of misrepresenting the signal’s underlying structure.
}{
This analysis examines how spectral summaries vary as a function of sampling resolution (number of samples) under the reported
generation settings. At lower resolutions, reduced sampling density and coarser frequency grids can obscure or merge spectral
peaks, complicating the separation of closely spaced spectral components. Higher resolutions provide finer frequency grids,
allowing spectral features to be represented with greater detail.
}
\\ \\
\noindent
\replacetext{
Ultimately, this evaluation seeks to determine the sampling resolution that optimizes both spectral fidelity and practical
utility. By quantifying the relationship between resolution and spectral stability, this approach provides a framework for
selecting appropriate sampling rates in real-world applications, ensuring accurate frequency-domain analysis while managing
computational resources efficiently.
}{
This evaluation documents how spectral characteristics change with sampling resolution, providing descriptive context for
using the dataset at different resolutions and computational budgets, rather than prescribing a universal sampling rate.
}
\\ \\
\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{graphs/psd.png}
    \caption{\replacetext{Average power spectral density (PSD) for different sampling resolutions based on 50 independent runs.}{Average power spectral density (PSD) for different sampling resolutions based on 50 independent runs (Hz axis under the illustrative convention $T=4\pi$\,s).}}
    \label{fig:average_psd}
\end{figure}
\noindent
\replacetext{
As shown in Figure~\ref{fig:average_psd}, lower sampling resolutions, specifically the blue curve (150 points) and the orange curve (250
points), exhibit a noticeable reduction in detail within the high-frequency range. These lower-resolution curves display greater
fluctuations and noise, particularly beyond 20 Hz, which is consistent with the theoretical effects of subsampling. The blue
curve (150 points) is especially affected, showing significant variability and a less stable spectral representation in the higher
frequencies.
}{
As shown in Figure~\ref{fig:average_psd}, lower sampling resolutions, such as the blue curve (150 samples) and the orange curve
(250 samples), exhibit a coarser spectral representation with increased fluctuations in higher normalized-frequency regions.
These patterns are consistent with the expected effects of subsampling and reduced frequency resolution. The 150-sample curve
shows greater variability across the upper portion of the spectrum under the same scaling.
}
\\ \\
\noindent
\replacetext{
In contrast, the higher sampling demonstrate a smoother and more stable spectral profile across all frequencies. The red curve
(1000 points), in particular, captures finer details and exhibits minimal high-frequency noise, making it the most reliable for
precise spectral analysis.
}{
In contrast, higher sampling resolutions demonstrate smoother spectral profiles across the frequency range. The red curve
(1000 samples), in particular, captures finer spectral structure and exhibits reduced high-frequency fluctuations, making it
the smoothest spectral estimate among the reported settings.
}

\subsection*{Impact of Noise on Frequency Characteristics}
\deltext{Analyzing the impact of noise on frequency characteristics is a critical step in validating the robustness and reliability of spectral analysis methods. Understanding how noise influences the Power Spectral Density (PSD) allows for the assessment of a method’s sensitivity and its ability to preserve essential signal features despite the presence of interference.}
\addtext{The effect of varying noise amplitude on the power spectral density (PSD) is analyzed, with particular attention to differences between low- and high-frequency regions.}
\\ 
\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{graphs/noise.png}
    \caption{Power spectral density (PSD) of signals generated with different noise amplitudes (Hz axis under the illustrative convention $T=4\pi$\,s).}
    \label{fig:noise_psd}
\end{figure}

\noindent \replacetext{Figure~\ref{fig:noise_psd} illustrates the impact of different noise amplitudes on the Power Spectral Density (PSD). As the noise amplitude increases—from 0.0 (blue curve) to 0.2 (red curve)—there is a noticeable rise in variability at higher frequencies, particularly beyond 10 Hz, while the low-frequency region remains comparatively stable.}{Figure~\ref{fig:noise_psd} illustrates the impact of different noise amplitudes on the Power Spectral Density (PSD) under the reported settings (Hz axis under the illustrative convention $T=4\pi$\,s). As the noise amplitude increases—from 0.0 (blue curve) to 0.2 (red curve)—the estimated PSD exhibits increased variability at higher frequencies, while the low-frequency region remains comparatively stable in these plots.}
\\ \\
\noindent
Across these settings, the low-frequency region changes less than the higher-frequency region in these estimates. This observation provides context for the subsequent super-resolution benchmark, where both time-domain and frequency-domain metrics are reported.


\section*{Usage Notes}
\label{sec:usage-notes}

\replacetext{
The CoSiBD dataset contains paired low- and high-resolution temporal signals in plain text format. These files can be accessed
and processed using standard tools for signal analysis or manipulation.
}{
The CoSiBD dataset contains high-resolution signals and corresponding subsampled versions at multiple resolutions. Signals are provided in consolidated \texttt{.txt}, \texttt{.npz}, and \texttt{.json} formats. Pairing between low- and high-resolution versions is performed by row index: row $i$ in a subsampled file corresponds to row $i$ in the high-resolution file, with per-signal parameters available in \texttt{signals\_metadata.json}.
}
\addtext{
The dataset is distributed as a single, unified collection without a predefined train/validation/test split. Users can create partitions appropriate to their objectives (e.g., random splits, stratified splits by noise type or signal characteristics, cross-validation, or scenario-specific test sets), using the provided metadata to support principled partitioning.
}
\\ \\
\subsection*{Reading the Data}

CoSiBD is distributed as consolidated plain-text (\texttt{.txt}) files in which each row corresponds to a single temporal signal (samples separated by whitespace). Low- and high-resolution signals are aligned by row index: row $i$ in a subsampled file corresponds to row $i$ in the high-resolution file. Per-signal generation parameters are provided in the accompanying metadata file (\texttt{signals\_metadata.json}) using the same indexing scheme.
\\ \\
The following example illustrates how to load paired low- and high-resolution signals using standard Python tools:

\begin{verbatim}
import numpy as np

# Load subsampled (simple decimation) and high-resolution signals
# Each .txt file is consolidated: one signal per row
x_lr = np.loadtxt('SignalBuilderC/data/signals_subsampled_simple_250.txt')
x_hr = np.loadtxt('SignalBuilderC/data/signals_high_resolution_5000.txt')

# Access a paired signal by row index
i = 0
low_res_signal  = x_lr[i]
high_res_signal = x_hr[i]
\end{verbatim}

These commands return NumPy arrays where each row corresponds to one signal. Users may optionally convert the arrays to other formats or frameworks depending on their analysis pipeline.

\subsection*{Visualizing Paired Signals}

To inspect the alignment between low- and high-resolution versions, users can visualize paired signals indexed by the same row:

\begin{verbatim}
import matplotlib.pyplot as plt
import numpy as np

# Visualize a paired low- and high-resolution signal
i = 0
plt.figure(figsize=(10, 4))

# High-resolution signal
plt.plot(high_res_signal, label='High-resolution (5000 samples)', alpha=0.8)

# Low-resolution signal (aligned to HR index range for visualization)
lr_x = np.linspace(0, len(high_res_signal), len(low_res_signal))
plt.scatter(lr_x, low_res_signal, color='red', s=12,
            label='Low-resolution (250 samples)')

plt.xlabel('Sample index')
plt.ylabel('Amplitude')
plt.title('Paired Low- and High-Resolution Signal')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
\end{verbatim}

This visualization highlights how the same underlying temporal structure is represented at different resolutions while preserving alignment between paired signals. Additional signal characteristics (e.g., change-points, frequency profiles, or noise configuration) can be retrieved from \texttt{signals\_metadata.json} using the same row index.

\section*{Code availability}
\label{sec:code-availability}

\noindent
The full signal generation pipeline used to create the CoSiBD dataset is openly available in a public GitHub repository:  
\href{https://github.com/DhamarAM/SignalBuilder/tree/main}{SignalBuilderC (CoSiBD scripts)}.
\\ \\
\noindent
The repository provides a modular Python package (\texttt{SignalBuilderC}) implementing all stages of the dataset construction process, including: (i) generation of high-resolution synthetic temporal signals with configurable frequency profiles and amplitude envelopes; (ii) deterministic creation of paired low-resolution signals via uniform subsampling; (iii) optional noise injection; and (iv) export of signals and associated metadata in NumPy (\texttt{.npz}), plain-text (\texttt{.txt}), and JSON (\texttt{.json}) formats. The codebase is documented and includes example scripts and notebooks illustrating dataset generation, regeneration from metadata, and basic data access.
\\ \\
\noindent
All source code is released under the MIT License, allowing reuse and extension of the generation framework for research and benchmarking purposes.

\vspace{0.3cm}
\noindent
The CoSiBD dataset itself is published separately on Zenodo and is cited in the Data Records section~\cite{cosibd_zenodo_2025}.  
The Zenodo record distributes the dataset under the Creative Commons Attribution 4.0 International (CC BY 4.0) license.



% ===== BIBLIOGRAPHY =====
% Using BibTeX with naturemag-doi style for Nature Scientific Data
% References stored in referencias.bib
% Note: \bibliographystyle{naturemag-doi} is defined in wlscirep.cls
\bibliography{referencias}

\section*{Acknowledgments}
This research was supported by Dean's Office of the Polytechnic College of the San Francisco de Quito University  and partially by ProyExcel-0069 project of the Andalusian University, Research and Innovation Department.

\section*{Author Contributions}
J. I. F. handled the methodological design for artificial data creation, probabilistic analysis, spline-based variations, noise distributions, and random node selection. J. A. L. was responsible for the time series methodological design. D. A. M. performed data processing and validation analysis. All of the authors have contributed to writing the manuscript.

\section*{Competing Interests}
The authors declare no competing interests.

\end{document}